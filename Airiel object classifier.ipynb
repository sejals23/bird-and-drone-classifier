{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[{"file_id":"1dgF0cZHS18SzvG8ScTAuJJrYavDhucSd","timestamp":1750065698201}],"collapsed_sections":["FJNUwmbgGyua","w6K7xa23Elo4","yQaldy8SH6Dl","mDgbUHAGgjLW","O_i_v8NEhb9l","HhfV-JJviCcP","Y3lxredqlCYt","3RnN4peoiCZX","x71ZqKXriCWQ","7hBIi_osiCS2","JlHwYmJAmNHm","35m5QtbWiB9F","PoPl-ycgm1ru","H0kj-8xxnORC","nA9Y7ga8ng1Z","PBTbrJXOngz2","u3PMJOP6ngxN","dauF4eBmngu3","bKJF3rekwFvQ","MSa1f5Uengrz","GF8Ens_Soomf","0wOQAZs5pc--","K5QZ13OEpz2H","lQ7QKXXCp7Bj","448CDAPjqfQr","KSlN3yHqYklG","t6dVpIINYklI","ijmpgYnKYklI","-JiQyfWJYklI","EM7whBJCYoAo","fge-S5ZAYoAp","85gYPyotYoAp","RoGjAbkUYoAp","4Of9eVA-YrdM","iky9q4vBYrdO","F6T5p64dYrdO","y-Ehk30pYrdP","bamQiAODYuh1","QHF8YVU7Yuh3","GwzvFGzlYuh3","qYpmQ266Yuh3","OH-pJp9IphqM","bbFf2-_FphqN","_ouA3fa0phqN","Seke61FWphqN","PIIx-8_IphqN","t27r6nlMphqO","r2jJGEOYphqO","b0JNsNcRphqO","BZR9WyysphqO","jj7wYXLtphqO","eZrbJ2SmphqO","rFu4xreNphqO","YJ55k-q6phqO","gCFgpxoyphqP","OVtJsKN_phqQ","lssrdh5qphqQ","U2RJ9gkRphqQ","1M8mcRywphqQ","tgIPom80phqQ","JMzcOPDDphqR","x-EpHcCOp1ci","X_VqEhTip1ck","8zGJKyg5p1ck","PVzmfK_Ep1ck","n3dbpmDWp1ck","ylSl6qgtp1ck","ZWILFDl5p1ck","M7G43BXep1ck","Ag9LCva-p1cl","E6MkPsBcp1cl","2cELzS2fp1cl","3MPXvC8up1cl","NC_X3p0fY2L0","UV0SzAkaZNRQ","YPEH6qLeZNRQ","q29F0dvdveiT","EXh0U9oCveiU","22aHeOlLveiV","g-ATYxFrGrvw","Yfr_Vlr8HBkt","8yEUt7NnHlrM","tEA2Xm5dHt1r","I79__PHVH19G","Ou-I18pAyIpj","fF3858GYyt-u","4_0_7-oCpUZd","hwyV_J3ipUZe","3yB-zSqbpUZe","dEUvejAfpUZe","Fd15vwWVpUZf","bn_IUdTipZyH","49K5P_iCpZyH","Nff-vKELpZyI","kLW572S8pZyI","dWbDXHzopZyI","yLjJCtPM0KBk","xiyOF9F70UgQ","7wuGOrhz0itI","id1riN9m0vUs","578E2V7j08f6","89xtkJwZ18nB","67NQN5KX2AMe","Iwf50b-R2tYG","GMQiZwjn3iu7","WVIkgGqN3qsr","XkPnILGE3zoT","Hlsf0x5436Go","mT9DMSJo4nBL","c49ITxTc407N","OeJFEK0N496M","9ExmJH0g5HBk","cJNqERVU536h","k5UmGsbsOxih","T0VqWOYE6DLQ","qBMux9mC6MCf","-oLEiFgy-5Pf","C74aWNz2AliB","2DejudWSA-a0","pEMng2IbBLp7","rAdphbQ9Bhjc","TNVZ9zx19K6k","nqoHp30x9hH9","rMDnDkt2B6du","yiiVWRdJDDil","1UUpS68QDMuG","kexQrXU-DjzY","T5CmagL3EC8N","BhH2vgX9EjGr","qjKvONjwE8ra","P1XJ9OREExlT","VFOzZv6IFROw","TIqpNgepFxVj","VfCC591jGiD4","OB4l2ZhMeS1U","ArJBuiUVfxKd","4qY1EAkEfxKe","PiV4Ypx8fxKe","TfvqoZmBfxKf","dJ2tPlVmpsJ0","JWYfwnehpsJ1","-jK_YjpMpsJ2","HAih1iBOpsJ2","zVGeBEFhpsJ2","bmKjuQ-FpsJ3","Fze-IPXLpx6K","7AN1z2sKpx6M","9PIHJqyupx6M","_-qAgymDpx6N","Z-hykwinpx6N","h_CCil-SKHpo","cBFFvTBNJzUa","HvGl1hHyA_VK","EyNgTHvd2WFk","KH5McJBi2d8v","iW_Lq9qf2h6X","-Kee-DAl2viO","gCX9965dhzqZ","gIfDvo9L0UH2"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Project Name**    -\n","\n"],"metadata":{"id":"vncDsAP0Gaoa"}},{"cell_type":"markdown","source":["##### **Project Type**    - Classification\n","##### **Contribution**    - Individual\n","##### **Name -** Sejal\n"],"metadata":{"id":"beRrZCGUAJYm"}},{"cell_type":"markdown","source":["# **Project Summary -**"],"metadata":{"id":"FJNUwmbgGyua"}},{"cell_type":"markdown","source":["The rapid expansion of aerial technology has significantly transformed modern security, surveillance, and environmental monitoring systems. With the increasing use of drones across commercial, industrial, and defense sectors, it has become crucial to differentiate drones from other flying entities—especially birds—during aerial surveillance. Misinterpretation between the two can lead to serious consequences, such as security breaches, airspace accidents, and ineffective wildlife monitoring. This project, Aerial Object Classification & Detection, aims to tackle this challenge using deep learning and computer vision. The objective is to develop an intelligent real-time solution that can classify aerial images into two categories—Bird or Drone—and optionally perform object detection to locate and label them within complex aerial scenes.\n","\n","The first phase of the project focuses on image classification using a Convolutional Neural Network (CNN). A custom CNN model was designed and trained on a curated dataset containing aerial images of birds and drones taken from various viewpoints, lighting conditions, and backgrounds. To improve robustness, the dataset underwent data preprocessing and augmentation techniques such as normalization, rotation, zooming, flipping, and contrast adjustments. These transformations ensured that the model learned to correctly identify objects even under environmental variations that occur in real applications.\n","\n","In addition to a custom CNN, the project leverages transfer learning to enhance accuracy and reduce training time. Pre-trained architectures such as ResNet, VGG, and EfficientNet were explored to reuse high-level visual feature extractors that have been trained on large-scale datasets. This step significantly improved generalization and reduced overfitting, making the model more scalable to real-world deployment.\n","\n","The second phase extends the project into real-time application through object detection. The latest YOLOv8 architecture was optionally implemented to detect and locate drones and birds within full aerial frames rather than just classifying single cropped images. This enables practical use cases such as timely alerts in restricted airspace, live runway monitoring, and automated wildlife tracking. YOLOv8 provides high frame-per-second performance, making it suitable for drone-mounted video surveillance and real-time defense applications.\n","\n","To measure performance, multiple evaluation metrics—including accuracy, precision, recall, F1-score, confusion matrix, and classification report—were generated. These metrics helped understand the strengths and limitations of both the custom CNN and transfer learning-based models. For the object detection module, mAP (mean Average Precision) served as the core metric. The results indicate that the system can reliably differentiate drones and birds across diverse aerial backgrounds, proving its readiness for real-world integration.\n","\n","Deployment was achieved using a Streamlit web interface to make the solution interactive and user-friendly. End-users can upload aerial images or provide video streams to receive classification results or detection visualizations. The front-end integrates seamlessly with the trained deep-learning model using Python APIs, allowing real-time inference without any complex manual setup. This feature enables rapid demonstration, prototyping, and potential field testing.\n","\n","This project offers immense business and societal benefits through its real-time use cases. In wildlife protection, it prevents collisions between wind turbines and birds, ensuring species conservation. For security and defense, it supports early detection of unauthorized drones entering restricted airspace—an area of growing global concern. In aviation safety, it can prevent costly and life-threatening aircraft bird strikes by monitoring runway zones. Additionally, in environmental research, it enhances large-scale tracking of bird populations using aerial imagery, providing more efficient alternatives to traditional manual observation.\n","\n","In conclusion, Aerial Object Classification & Detection is a holistic AI-driven solution that integrates image classification, deep learning, object detection, and deployment technologies. It demonstrates how computer vision can significantly improve both human safety and wildlife conservation when applied to real-time aerial surveillance. With future enhancements such as drone-mounted inference, edge-device optimization, and multi-class detection, the project has strong potential for large-scale industrial deployment and commercial adoption."],"metadata":{"id":"F6v_1wHtG2nS"}},{"cell_type":"markdown","source":["# **GitHub Link -**"],"metadata":{"id":"w6K7xa23Elo4"}},{"cell_type":"markdown","source":[],"metadata":{"id":"h1o69JH3Eqqn"}},{"cell_type":"markdown","source":["# **Problem Statement**\n"],"metadata":{"id":"yQaldy8SH6Dl"}},{"cell_type":"markdown","source":["The project aims to develop an AI-based system that can accurately differentiate drones from birds in aerial images and videos. Misclassification currently leads to security, aviation, and wild"],"metadata":{"id":"DpeJGUA3kjGy"}},{"cell_type":"markdown","source":["# ***Let's Begin !***"],"metadata":{"id":"O_i_v8NEhb9l"}},{"cell_type":"markdown","source":["## ***1. Know Your Data***"],"metadata":{"id":"HhfV-JJviCcP"}},{"cell_type":"markdown","source":["### Import Libraries"],"metadata":{"id":"Y3lxredqlCYt"}},{"cell_type":"code","source":["# Import Libraries\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import warnings\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","warnings.filterwarnings(\"ignore\")"],"metadata":{"id":"M8Vqi-pPk-HR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataset Loading"],"metadata":{"id":"3RnN4peoiCZX"}},{"cell_type":"code","source":["# Load Dataset\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","\n","dataset_path = \"/content/drive/MyDrive/BirdandDroneclassification/\"\n","\n","# Image size & batch\n","IMG_SIZE = (224, 224)\n","BATCH_SIZE = 32\n","\n","# Data Augmentation for training set\n","train_datagen = ImageDataGenerator(\n","    rescale = 1./255,\n","    rotation_range = 20,\n","    zoom_range = 0.2,\n","    horizontal_flip = True\n",")\n","\n","# No augmentation for validation & testing\n","valid_test_datagen = ImageDataGenerator(rescale = 1./255)\n","\n","# Loading training data\n","train_data = train_datagen.flow_from_directory(\n","    dataset_path + \"train/\",\n","    target_size = IMG_SIZE,\n","    batch_size = BATCH_SIZE,\n","    class_mode = \"categorical\"\n",")\n","\n","# Loading validation data\n","valid_data = valid_test_datagen.flow_from_directory(\n","    dataset_path + \"valid/\",\n","    target_size = IMG_SIZE,\n","    batch_size = BATCH_SIZE,\n","    class_mode = \"categorical\"\n",")\n","\n","# Loading test data\n","test_data = valid_test_datagen.flow_from_directory(\n","    dataset_path + \"test/\",\n","    target_size = IMG_SIZE,\n","    batch_size = BATCH_SIZE,\n","    class_mode = \"categorical\",\n","    shuffle = False\n",")\n","\n","print(\"Class Index Mapping:\", train_data.class_indices)\n","\n"],"metadata":{"id":"4CkvbW_SlZ_R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataset First View"],"metadata":{"id":"x71ZqKXriCWQ"}},{"cell_type":"code","source":["# Dataset First Look\n","images, labels = next(train_data)\n","\n","# Number of images to display\n","num_images = 9\n","\n","plt.figure(figsize=(10, 10))\n","for i in range(num_images):\n","    plt.subplot(3, 3, i + 1)\n","    plt.imshow(images[i])\n","    plt.title(\"Bird\" if np.argmax(labels[i]) == 0 else \"Drone\")\n","    plt.axis(\"off\")\n","\n","plt.suptitle(\"Sample Images from Train Set\", fontsize=14)\n","plt.show()"],"metadata":{"id":"LWNFOSvLl09H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3. ***Data Wrangling***"],"metadata":{"id":"dauF4eBmngu3"}},{"cell_type":"markdown","source":["### Data Wrangling Code"],"metadata":{"id":"bKJF3rekwFvQ"}},{"cell_type":"code","source":["# Normalizing the dataset\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","# Normalization factor\n","NORMALIZE = 1.0 / 255.0\n","\n","train_datagen = ImageDataGenerator(\n","    rescale = NORMALIZE,       # ✔ Normalizes pixel values\n","    rotation_range = 20,\n","    zoom_range = 0.2,\n","    horizontal_flip = True\n",")\n","\n","valid_test_datagen = ImageDataGenerator(\n","    rescale = NORMALIZE        # ✔ Normalizes validation & test images\n",")\n","\n","# Loading normalized datasets\n","train_data = train_datagen.flow_from_directory(\n","    dataset_path + \"train/\",\n","    target_size = IMG_SIZE,\n","    batch_size = BATCH_SIZE,\n","    class_mode = \"categorical\"\n",")\n","\n","valid_data = valid_test_datagen.flow_from_directory(\n","    dataset_path + \"valid/\",\n","    target_size = IMG_SIZE,\n","    batch_size = BATCH_SIZE,\n","    class_mode = \"categorical\"\n",")\n","\n","test_data = valid_test_datagen.flow_from_directory(\n","    dataset_path + \"test/\",\n","    target_size = IMG_SIZE,\n","    batch_size = BATCH_SIZE,\n","    class_mode = \"categorical\",\n","    shuffle = False\n",")\n"],"metadata":{"id":"wk-9a2fpoLcV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Data augmentation\n","train_datagen = ImageDataGenerator(\n","    rescale = 1/255,\n","    rotation_range = 20,\n","    width_shift_range = 0.2,\n","    height_shift_range = 0.2,\n","    zoom_range = 0.2,\n","    shear_range = 0.2,\n","    horizontal_flip = True\n",")\n"],"metadata":{"id":"Zrf8GnVZU4Tx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### What all manipulations have you done and insights you found?"],"metadata":{"id":"MSa1f5Uengrz"}},{"cell_type":"markdown","source":["Answer Here."],"metadata":{"id":"LbyXE7I1olp8"}},{"cell_type":"markdown","source":["## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"],"metadata":{"id":"GF8Ens_Soomf"}},{"cell_type":"markdown","source":["#### Chart - 1"],"metadata":{"id":"0wOQAZs5pc--"}},{"cell_type":"code","source":["# Chart - 1 visualization code\n","import os\n","\n","class_names = train_data.class_indices\n","class_count = {cls: 0 for cls in class_names}\n","\n","for cls in class_names:\n","    class_path = os.path.join(\"/content/drive/MyDrive/BirdandDroneclassification/train\", cls)\n","    class_count[cls] = len(os.listdir(class_path))\n","\n","plt.bar(class_count.keys(), class_count.values())\n","plt.title(\"Class Distribution in Training Data\")\n","plt.xticks(rotation=45)\n","plt.show()"],"metadata":{"id":"7v_ESjsspbW7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"K5QZ13OEpz2H"}},{"cell_type":"markdown","source":["Answer Here."],"metadata":{"id":"XESiWehPqBRc"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"lQ7QKXXCp7Bj"}},{"cell_type":"markdown","source":["Answer Here"],"metadata":{"id":"C_j1G7yiqdRP"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"448CDAPjqfQr"}},{"cell_type":"markdown","source":["Answer Here"],"metadata":{"id":"3cspy4FjqxJW"}},{"cell_type":"markdown","source":["#### Chart - 2"],"metadata":{"id":"KSlN3yHqYklG"}},{"cell_type":"code","source":["# Chart - 2 visualization code\n","import numpy as np\n","from PIL import Image\n","\n","img_path = train_data.filepaths[0]\n","img = Image.open(img_path)\n","print(\"Image size:\", np.array(img).shape)\n"],"metadata":{"id":"R4YgtaqtYklH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"t6dVpIINYklI"}},{"cell_type":"markdown","source":["Answer Here."],"metadata":{"id":"5aaW0BYyYklI"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"ijmpgYnKYklI"}},{"cell_type":"markdown","source":["Answer Here"],"metadata":{"id":"PSx9atu2YklI"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"-JiQyfWJYklI"}},{"cell_type":"markdown","source":["Answer Here"],"metadata":{"id":"BcBbebzrYklV"}},{"cell_type":"markdown","source":["#### Chart - 3"],"metadata":{"id":"EM7whBJCYoAo"}},{"cell_type":"code","source":["# Chart - 3 visualization code\n","import cv2\n","\n","img = cv2.imread(img_path)\n","plt.hist(img.ravel(), bins=255)\n","plt.title(\"Pixel Intensity Distribution\")\n","plt.show()\n"],"metadata":{"id":"t6GMdE67YoAp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"fge-S5ZAYoAp"}},{"cell_type":"markdown","source":["Answer Here."],"metadata":{"id":"5dBItgRVYoAp"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"85gYPyotYoAp"}},{"cell_type":"markdown","source":["Answer Here"],"metadata":{"id":"4jstXR6OYoAp"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"RoGjAbkUYoAp"}},{"cell_type":"markdown","source":["Answer Here"],"metadata":{"id":"zfJ8IqMcYoAp"}},{"cell_type":"markdown","source":["## ***7. ML Model Implementation***"],"metadata":{"id":"VfCC591jGiD4"}},{"cell_type":"markdown","source":["### ML Model - 1"],"metadata":{"id":"OB4l2ZhMeS1U"}},{"cell_type":"code","source":["# ML Model - 1 Implementation\n","\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense, Dropout\n","\n","model = Sequential([\n","    Conv2D(32, (3,3), activation='relu', input_shape=(224, 224, 3)),\n","    MaxPool2D(2,2),\n","\n","    Conv2D(64, (3,3), activation='relu'),\n","    MaxPool2D(2,2),\n","\n","    Conv2D(128, (3,3), activation='relu'),\n","    MaxPool2D(2,2),\n","\n","    Flatten(),\n","    Dense(128, activation='relu'),\n","    Dropout(0.5),\n","    Dense(2, activation='softmax')\n","])\n","\n","model.compile(\n","    optimizer='adam',\n","    loss='categorical_crossentropy',\n","    metrics=['accuracy']\n",")\n","model.summary()\n"],"metadata":{"id":"7ebyywQieS1U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Train the model"],"metadata":{"id":"7zogVzveVVDe"}},{"cell_type":"code","source":["history = model.fit(\n","    train_data,\n","    validation_data = test_data,\n","    epochs = 20,\n","    batch_size = 32\n",")\n"],"metadata":{"id":"WrRz6b2nVSRT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Evaluate the model"],"metadata":{"id":"jlwc9If1V8PX"}},{"cell_type":"code","source":["loss, acc = model.evaluate(test_data)\n","print(\"Test Accuracy:\", acc)\n"],"metadata":{"id":"bmprAkA8V4ZB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import classification_report\n","import numpy as np\n","\n","pred = model.predict(test_data)\n","y_pred = np.argmax(pred, axis=1)\n","print(classification_report(test_data.classes, y_pred))\n"],"metadata":{"id":"GcWJa-8JWDLg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."],"metadata":{"id":"ArJBuiUVfxKd"}},{"cell_type":"code","source":["# Visualizing evaluation Metric Score chart\n","# Accuracy curve\n","plt.plot(history.history['accuracy'])\n","plt.plot(history.history['val_accuracy'])\n","plt.title(\"Model Accuracy\")\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Accuracy\")\n","plt.legend([\"Train Accuracy\", \"Validation Accuracy\"])\n","plt.show()\n","\n","# Loss curve\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title(\"Model Loss\")\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Loss\")\n","plt.legend([\"Train Loss\", \"Validation Loss\"])\n","plt.show()"],"metadata":{"id":"rqD5ZohzfxKe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Conclusion**"],"metadata":{"id":"gCX9965dhzqZ"}},{"cell_type":"markdown","source":["his project successfully developed an AI-based image classification system capable of distinguishing Birds and Drones from aerial images using a Convolutional Neural Network (CNN). The model learned meaningful spatial patterns such as edges, textures, and shapes directly from the dataset through normalization and augmentation, ensuring reliable feature extraction and improved generalization. With an overall accuracy of 80%, supported by strong F1-score and precision–recall performance, the model demonstrates effective classification capability, especially in accurately detecting drones — a critical requirement for security surveillance, aviation safety, and wildlife monitoring."],"metadata":{"id":"Fjb1IsQkh3yE"}},{"cell_type":"markdown","source":["### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"],"metadata":{"id":"gIfDvo9L0UH2"}}]}